
import React, { createContext, useState, useContext, ReactNode, useRef, useEffect } from 'react';
import * as webllm from "@mlc-ai/web-llm";
import { useHistory } from './HistoryContext';
import { useSubscription } from './SubscriptionContext';
import { usePreference } from './PreferenceContext';
import { inferTopInterests, buildUserProfile } from '../utils/xrai';

// Use Phi-3.5-mini-instruct for high performance (12B equivalent reasoning) with low VRAM usage
const SELECTED_MODEL = "Phi-3.5-mini-instruct-q4f16_1-MLC"; 

interface ChatMessage {
  role: 'user' | 'assistant' | 'system';
  content: string;
}

interface AiContextType {
  messages: ChatMessage[];
  isLoaded: boolean;
  isLoading: boolean;
  loadProgress: string;
  sendMessage: (text: string) => Promise<void>;
  initializeEngine: () => Promise<void>;
  resetChat: () => void;
}

const AiContext = createContext<AiContextType | undefined>(undefined);

export const AiProvider: React.FC<{ children: ReactNode }> = ({ children }) => {
  const [messages, setMessages] = useState<ChatMessage[]>([]);
  const [isLoaded, setIsLoaded] = useState(false);
  const [isLoading, setIsLoading] = useState(false);
  const [loadProgress, setLoadProgress] = useState('');
  
  const engine = useRef<webllm.MLCEngine | null>(null);
  const { history } = useHistory();
  const { subscribedChannels } = useSubscription();
  const { preferredGenres } = usePreference();

  // Construct system prompt based on XRAI profile
  const getSystemPrompt = () => {
    const profile = buildUserProfile({
        watchHistory: history,
        searchHistory: [], // Can add if available in context
        subscribedChannels: subscribedChannels
    });
    const interests = inferTopInterests(profile, 10);
    const userContext = `
    User Interests: ${interests.join(', ')}
    Preferred Genres: ${preferredGenres.join(', ')}
    Recent History: ${history.slice(0, 5).map(v => v.title).join(', ')}
    `;

    return `ã‚ãªãŸã¯å‹•ç”»å…±æœ‰ã‚µã‚¤ãƒˆã€ŒXeroxYTã€ã®AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦–è´å±¥æ­´ã‚„å¥½ã¿ã‚’åˆ†æžã—ã€è¦ªã—ã¿ã‚„ã™ã„æ—¥æœ¬èªžã§ä¼šè©±ã—ã¦ãã ã•ã„ã€‚
    ã‚ãªãŸã®å½¹å‰²ã¯ã€å‹•ç”»ã®æŽ¨è–¦ã€é›‘è«‡ã€ãã—ã¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®èˆˆå‘³ã‚’æ·±æŽ˜ã‚Šã™ã‚‹ã“ã¨ã§ã™ã€‚
    
    [ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±]
    ${userContext}
    
    å›žç­”ã¯çŸ­ãã€ç°¡æ½”ã«ã€ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ã«ã—ã¦ãã ã•ã„ã€‚çµµæ–‡å­—ã‚‚é©åº¦ã«ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚`;
  };

  const initializeEngine = async () => {
    if (engine.current || isLoading) return;
    
    setIsLoading(true);
    setLoadProgress('ã‚¨ãƒ³ã‚¸ãƒ³ã®åˆæœŸåŒ–ä¸­...');

    try {
      const initProgressCallback = (report: webllm.InitProgressReport) => {
        setLoadProgress(report.text);
      };

      const newEngine = await webllm.CreateMLCEngine(
        SELECTED_MODEL,
        { initProgressCallback: initProgressCallback }
      );

      engine.current = newEngine;
      setIsLoaded(true);
      
      // Initial greeting
      const initialMsg = { role: 'assistant' as const, content: 'ã“ã‚“ã«ã¡ã¯ï¼XeroxYTã®AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ðŸ‘‹\nã‚ãªãŸã®å¥½ã¿ã«åˆã‚ã›ãŸå‹•ç”»æŽ¢ã—ã‚’ãŠæ‰‹ä¼ã„ã—ã¾ã™ã€‚ä½•ã‹èžããŸã„ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ' };
      setMessages([initialMsg]);

    } catch (error) {
      console.error("Failed to load WebLLM:", error);
      setLoadProgress('AIã‚¨ãƒ³ã‚¸ãƒ³ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸã€‚WebGPUå¯¾å¿œãƒ–ãƒ©ã‚¦ã‚¶ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚');
    } finally {
      setIsLoading(false);
    }
  };

  const sendMessage = async (text: string) => {
    if (!engine.current || !text.trim()) return;

    const userMsg: ChatMessage = { role: 'user', content: text };
    setMessages(prev => [...prev, userMsg]);

    try {
      // Dynamic System Prompt injection
      const historyMsgs = messages.map(m => ({ role: m.role, content: m.content }));
      const prompt = [
          { role: 'system', content: getSystemPrompt() },
          ...historyMsgs,
          userMsg
      ];

      const reply = await engine.current.chat.completions.create({
        messages: prompt as any,
        temperature: 0.7,
        max_tokens: 256, // Keep responses concise
      });

      const assistantMsg: ChatMessage = { 
        role: 'assistant', 
        content: reply.choices[0].message.content || 'ã™ã¿ã¾ã›ã‚“ã€ã†ã¾ãç­”ãˆã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚' 
      };
      
      setMessages(prev => [...prev, assistantMsg]);
    } catch (error) {
      console.error("Chat generation failed:", error);
      setMessages(prev => [...prev, { role: 'assistant', content: 'ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚‚ã†ä¸€åº¦è©¦ã—ã¦ãã ã•ã„ã€‚' }]);
    }
  };

  const resetChat = () => {
      if(engine.current) {
          engine.current.resetChat();
      }
      setMessages([]);
      initializeEngine(); // Re-init to send greeting
  };

  return (
    <AiContext.Provider value={{ messages, isLoaded, isLoading, loadProgress, sendMessage, initializeEngine, resetChat }}>
      {children}
    </AiContext.Provider>
  );
};

export const useAi = (): AiContextType => {
  const context = useContext(AiContext);
  if (context === undefined) {
    throw new Error('useAi must be used within an AiProvider');
  }
  return context;
};
